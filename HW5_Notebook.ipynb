{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "coms-w4995",
      "language": "python",
      "name": "coms-w4995"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.8"
    },
    "colab": {
      "name": "Applied_ML_HW5_NLP.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d9bcdbd2-3401-41ad-a83f-830e9346e607"
      },
      "source": [
        "# **Applied Machine Learning Homework 5**\n",
        "**Due 2 May,2022 (Monday) 11:59PM EST**"
      ],
      "id": "d9bcdbd2-3401-41ad-a83f-830e9346e607"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "70df26be-5638-4b0d-a252-4437eb76aa46"
      },
      "source": [
        "### Natural Language Processing\n",
        "We will train a supervised training model to predict if a tweet has a positive or negative sentiment."
      ],
      "id": "70df26be-5638-4b0d-a252-4437eb76aa46"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2e0d9a19-25ea-4490-b0e8-7909bcdc3d9d"
      },
      "source": [
        "####  **Dataset loading & dev/test splits**"
      ],
      "id": "2e0d9a19-25ea-4490-b0e8-7909bcdc3d9d"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fafa37c4-c8fc-4697-9bbe-11539d710bf7"
      },
      "source": [
        "**1.1) Load the twitter dataset from NLTK library**"
      ],
      "id": "fafa37c4-c8fc-4697-9bbe-11539d710bf7"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5f4ce405-237b-42d2-9c81-25ff28deaf4a",
        "outputId": "07278bb8-dd90-4720-b00c-3c07c9c9604c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import nltk\n",
        "nltk.download('twitter_samples')\n",
        "from nltk.corpus import twitter_samples "
      ],
      "id": "5f4ce405-237b-42d2-9c81-25ff28deaf4a",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package twitter_samples to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/twitter_samples.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c41d62ce-3c78-4b6c-9238-111d990d170f"
      },
      "source": [
        "**1.2) Load the positive & negative tweets**"
      ],
      "id": "c41d62ce-3c78-4b6c-9238-111d990d170f"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b92fb408-f72a-4c23-acd8-7c944a52edd3"
      },
      "source": [
        "all_positive_tweets = twitter_samples.strings('positive_tweets.json')\n",
        "all_negative_tweets = twitter_samples.strings('negative_tweets.json')"
      ],
      "id": "b92fb408-f72a-4c23-acd8-7c944a52edd3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "12eae071-fd8a-4a46-9958-0525c635fd88"
      },
      "source": [
        "**1.3) Create a development & test split (80/20 ratio):**"
      ],
      "id": "12eae071-fd8a-4a46-9958-0525c635fd88"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0f3673db-d7a8-470b-a3d3-f4522cd359b8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "83415f7a-8063-47da-b25b-fd3a60282c20"
      },
      "source": [
        "#code here\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "tweets = []\n",
        "tweets = all_positive_tweets + all_negative_tweets\n",
        "\n",
        "print(len(tweets))\n",
        "\n",
        "y     = []\n",
        "y_pos = [1]*len(all_positive_tweets)\n",
        "y_neg = [0]*len(all_negative_tweets)\n",
        "y     = y_pos + y_neg\n",
        "print(len(y))\n",
        "\n",
        "\n",
        "dev_text,test_text,dev_y,test_y = train_test_split(tweets, y, test_size = 0.2, random_state = 42)"
      ],
      "id": "0f3673db-d7a8-470b-a3d3-f4522cd359b8",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10000\n",
            "10000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "32b23398-e80e-4624-b89e-c02fabfd3f8d"
      },
      "source": [
        "#### **Data preprocessing**\n",
        "We will do some data preprocessing before we tokenize the data. We will remove `#` symbol, hyperlinks, stop words & punctuations from the data. You can use the `re` package in python to find and replace these strings. "
      ],
      "id": "32b23398-e80e-4624-b89e-c02fabfd3f8d"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f89d9d69-1640-4583-a7b7-7ec04ccf3310"
      },
      "source": [
        "**1.4) Replace the `#` symbol with '' in every tweet**"
      ],
      "id": "f89d9d69-1640-4583-a7b7-7ec04ccf3310"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5db4dd6d-e775-49d3-96e1-57620c042d46"
      },
      "source": [
        "#code here\n",
        "dev_text_1 = [string.replace(\"#\", \"\") for string in dev_text]\n",
        "test_text_1 = [string.replace(\"#\", \"\") for string in test_text]\n"
      ],
      "id": "5db4dd6d-e775-49d3-96e1-57620c042d46",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "24c4caa8-d71d-46a8-8859-a8e85c56acfe"
      },
      "source": [
        "**1.5) Replace hyperlinks with '' in every tweet**"
      ],
      "id": "24c4caa8-d71d-46a8-8859-a8e85c56acfe"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ff5a7411-df49-427b-adef-5e8e63224db0"
      },
      "source": [
        "import re\n",
        "\n",
        "dev_text_2 = []\n",
        "test_text_2 = []\n",
        "\n",
        "for string in dev_text_1:\n",
        "    dev_text_2.append(re.sub(r\"\\bhttp\\S+\\b\", \"\", string))\n",
        "\n",
        "for string in test_text_1:\n",
        "    test_text_2.append(re.sub(r\"\\bhttp\\S+\\b\", \"\", string))"
      ],
      "id": "ff5a7411-df49-427b-adef-5e8e63224db0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "492ae463-b611-4292-9ad2-b778856bf8bc"
      },
      "source": [
        "**1.6) Remove all stop words**"
      ],
      "id": "492ae463-b611-4292-9ad2-b778856bf8bc"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "961d73fd-a662-46f2-85a2-83bf6b978189",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "164a9f71-0354-43b6-8e13-4cfe300707c5"
      },
      "source": [
        "#from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "#nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('punkt')\n",
        "\n",
        "#from sklearn.feature_extraction.text import CountVectorizer\n",
        "# vector_stop_words = CountVectorizer(stop_words='english')\n",
        "# dev_text_3 = vector_stop_words.fit_transform(dev_text_2)\n",
        "# test_text_3 = vector_stop_words.transform(test_text_2)\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "dev_text_3 = []\n",
        "for tweet in dev_text_2:\n",
        "    text_tokens = word_tokenize(tweet)\n",
        "    for word in text_tokens:\n",
        "        tokens_without_sw = [word for word in text_tokens if not word in stop_words]\n",
        "    filtered_sentence = (\" \").join(tokens_without_sw)\n",
        "    dev_text_3.append(filtered_sentence)\n",
        "\n",
        "test_text_3 = []\n",
        "for tweet in test_text_2:\n",
        "    text_tokens = word_tokenize(tweet)\n",
        "    for word in text_tokens:\n",
        "        tokens_without_sw = [word for word in text_tokens if not word in stop_words]\n",
        "    filtered_sentence = (\" \").join(tokens_without_sw)\n",
        "    test_text_3.append(filtered_sentence)\n",
        "\n",
        "print(len(dev_text_3))\n",
        "print(len(test_text_3))"
      ],
      "id": "961d73fd-a662-46f2-85a2-83bf6b978189",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "8000\n",
            "2000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "169bf8ad-f7ba-4e67-a1a0-92fcdd193ab9"
      },
      "source": [
        "**1.7) Remove all punctuations**"
      ],
      "id": "169bf8ad-f7ba-4e67-a1a0-92fcdd193ab9"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "774743e0-8cf0-4dbb-a6fa-006ff076bb9e"
      },
      "source": [
        "#code here\n",
        "dev_text_4  = []\n",
        "test_text_4 = []\n",
        "for string in dev_text_3:\n",
        "    dev_text_4.append(re.sub(r'[^\\w\\s]', '', string))\n",
        "\n",
        "for string in test_text_3:\n",
        "    test_text_4.append(re.sub(r'[^\\w\\s]', '', string))\n"
      ],
      "id": "774743e0-8cf0-4dbb-a6fa-006ff076bb9e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b2f1af18-0c07-4ffb-994e-daead4740a53"
      },
      "source": [
        "**1.8) Apply stemming on the development & test datasets using Porter algorithm**"
      ],
      "id": "b2f1af18-0c07-4ffb-994e-daead4740a53"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c84a52f6-a62a-4033-8d1d-239ff6904248"
      },
      "source": [
        "#code here\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "porter = PorterStemmer()\n",
        "def stemSentence(sentence):\n",
        "    token_words = word_tokenize(sentence)\n",
        "    stem_sentence = [porter.stem(word) for word in token_words]\n",
        "    return \" \".join(stem_sentence)\n",
        "\n",
        "dev_text_5 = [stemSentence(review) for review in dev_text_4]\n",
        "test_text_5 = [stemSentence(review) for review in test_text_4]"
      ],
      "id": "c84a52f6-a62a-4033-8d1d-239ff6904248",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(dev_text_5[:3])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kHE2j8d8o0GP",
        "outputId": "74ca02b1-0195-41a6-cfd2-6801e81f796a"
      },
      "id": "kHE2j8d8o0GP",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['matt', 'lachdog_au posica good thank anyway', 'bf mean']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "687e23ef-dafd-4183-b2f1-86089e281dd8"
      },
      "source": [
        "#### **Model training**"
      ],
      "id": "687e23ef-dafd-4183-b2f1-86089e281dd8"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0c40fa44-01ad-4788-98b9-9c8f0c1252ef"
      },
      "source": [
        "**1.9) Create bag of words features for each tweet in the development dataset**"
      ],
      "id": "0c40fa44-01ad-4788-98b9-9c8f0c1252ef"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c17c6b99-9dfb-4d30-9e03-d596a9da880a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f2922180-456a-4293-fd59-51e0f04d2fcd"
      },
      "source": [
        "#code here\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "vector = CountVectorizer()\n",
        "dev_X  = vector.fit_transform(dev_text_5)\n",
        "test_X = vector.transform(test_text_5)\n",
        "\n",
        "dev_X\n"
      ],
      "id": "c17c6b99-9dfb-4d30-9e03-d596a9da880a",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<8000x14570 sparse matrix of type '<class 'numpy.int64'>'\n",
              "\twith 52332 stored elements in Compressed Sparse Row format>"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4baf65cd-019b-4ff4-b93c-3ca8cfffca8e"
      },
      "source": [
        "**1.10) Train a supervised learning model of choice on the development dataset**"
      ],
      "id": "4baf65cd-019b-4ff4-b93c-3ca8cfffca8e"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3433a6b0-408d-462e-9072-3495b21bc97b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7e3f7c4e-d007-4e96-a2e3-6c458a0f6d23"
      },
      "source": [
        "#code here\n",
        "from sklearn.linear_model import LogisticRegressionCV\n",
        "\n",
        "lr = LogisticRegressionCV(max_iter=1000).fit(dev_X,dev_y)\n",
        "lr.score(test_X,test_y)"
      ],
      "id": "3433a6b0-408d-462e-9072-3495b21bc97b",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.749"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1c16c6f6-7ab2-4d7a-b9dc-098a72381340"
      },
      "source": [
        "**1.11) Create TF-IDF features for each tweet in the development dataset**"
      ],
      "id": "1c16c6f6-7ab2-4d7a-b9dc-098a72381340"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7b417843-ffc4-4614-b2ef-964f8ec3e510"
      },
      "source": [
        "#code here\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "vector = TfidfVectorizer()\n",
        "dev_X_2  = vector.fit_transform(dev_text_5)\n",
        "test_X_2 = vector.transform(test_text_5)"
      ],
      "id": "7b417843-ffc4-4614-b2ef-964f8ec3e510",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ea3c9776-aad9-4eda-b3c2-d9f6b3e52427"
      },
      "source": [
        "**1.12) Train the same supervised learning algorithm on the development dataset with TF-IDF features**"
      ],
      "id": "ea3c9776-aad9-4eda-b3c2-d9f6b3e52427"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b8c7fe8b-61de-4daa-a338-74295a4902ce",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "efe51442-1a1b-4919-e028-d40c3c0e60bc"
      },
      "source": [
        "#code here\n",
        "lr = LogisticRegressionCV(max_iter=1000).fit(dev_X_2,dev_y)\n",
        "lr.score(test_X_2,test_y)"
      ],
      "id": "b8c7fe8b-61de-4daa-a338-74295a4902ce",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.763"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ab0129e7-a0ea-473e-9ad1-667b44a13a92"
      },
      "source": [
        "**1.13) Compare the performance of the two models on the test dataset**"
      ],
      "id": "ab0129e7-a0ea-473e-9ad1-667b44a13a92"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a64ca176-dab8-4965-a85d-dcf9dc013717"
      },
      "source": [
        "#code here\n",
        "# The TF-IDF features improve the performance by 1.4% !"
      ],
      "id": "a64ca176-dab8-4965-a85d-dcf9dc013717",
      "execution_count": null,
      "outputs": []
    }
  ]
}